import os
import tempfile
import shutil
import asyncio
from typing import List, Tuple
from app.database import get_collections
from app.database.enums import DocumentJobStatus, DocumentType, DocumentStatus
from app.database.models import DocumentJobModel, DocumentModel, DocumentChunkModel
from app.database.value_objects import DocumentFile
from app.storage import MinioManager
from core.cqrs import CommandHandler, CommandRegistry
from bson import ObjectId
from core.embedding import EmbeddingModel
from rag.parser import parse_file
from rag.chunking import Chunker
from shared.messages import DocumentMessage
from app.nlp.diabetes_classifier import DiabetesClassifier
from ..process_document_upload_command import ProcessDocumentUploadCommand
from core.result import Result
from utils import get_logger, FileHashUtils


@CommandRegistry.register_handler(ProcessDocumentUploadCommand)
class ProcessDocumentUploadCommandHandler(CommandHandler):

    def __init__(self):
        super().__init__()
        self.logger = get_logger(__name__)
        self.collections = get_collections()
        self.minio_manager = MinioManager.get_instance()
        self.diabetes_classifier = DiabetesClassifier()
        self.file_hash_utils = FileHashUtils()

    async def execute(self, command: ProcessDocumentUploadCommand) -> Result[None]:
        """Th·ª±c thi x·ª≠ l√Ω upload t√†i li·ªáu m·ªôt c√°ch b·∫•t ƒë·ªìng b·ªô"""
        if not command.document_job_id or not ObjectId.is_valid(command.document_job_id):
            return Result.failure(message="ID c√¥ng vi·ªác t√†i li·ªáu kh√¥ng h·ª£p l·ªá", code="INVALID_INPUT")

        job_id = command.document_job_id
        self.logger.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t√†i li·ªáu: {job_id}")

        existing_job = await self.collections.document_jobs.find_one({"_id": ObjectId(job_id)})
        if not existing_job:
            return Result.failure(message=DocumentMessage.NOT_FOUND.message, code=DocumentMessage.NOT_FOUND.code)

        status = existing_job.get("processing_status", {}).get("status")
        if status in [DocumentJobStatus.PROCESSING, DocumentJobStatus.COMPLETED, DocumentJobStatus.FAILED]:
            self.logger.warning(f"Job ƒë√£ ·ªü tr·∫°ng th√°i {status}, b·ªè qua: {job_id}")
            return Result.failure(message="Job ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω", code="ALREADY_PROCESSED")

        temp_dir = None
        try:
            embedding_model = await EmbeddingModel.get_instance()
            self.chunker = Chunker(
                embedding_model=embedding_model,
                tokenizer=embedding_model.model.tokenizer,
                max_tokens=512,
                min_tokens=100,
                overlap_tokens=64,
                similarity_threshold=0.6,
            )

            # L·∫•y th√¥ng tin document job
            document_job = await self._get_document_job(job_id)
            if not document_job:
                return Result.failure(
                    message=DocumentMessage.NOT_FOUND.message,
                    code=DocumentMessage.NOT_FOUND.code
                )

            # T·∫£i file v·ªÅ
            temp_dir, temp_path = await self._download_file_async(job_id, document_job.file.path)

            # Ki·ªÉm tra tr√πng file b·∫±ng hash
            if await self._is_duplicate_async(job_id, temp_path):
                return Result.failure(
                    message=DocumentMessage.DUPLICATE.message,
                    code=DocumentMessage.DUPLICATE.code
                )

            # X·ª≠ l√Ω n·ªôi dung
            content = await self._parse_content_async(job_id, temp_path)
            chunks = await self._create_chunks_async(job_id, content)
            scores = await self._score_chunks_async(job_id, chunks)

            # L∆∞u document v√† chunks (kh√¥ng x√≥a c≈©)
            await self._save_document_async(job_id, document_job, temp_path, chunks, scores)

            # C·∫≠p nh·∫≠t tr·∫°ng th√°i th√†nh c√¥ng
            await self._update_status_async(
                job_id=job_id,
                status=DocumentJobStatus.COMPLETED,
                progress=100,
                priority_diabetes=sum(scores) / len(scores) if scores else 0.0,
                file_size=os.path.getsize(temp_path),
                message="Ho√†n t·∫•t x·ª≠ l√Ω",
            )

            self.logger.info(f"X·ª≠ l√Ω t√†i li·ªáu th√†nh c√¥ng: {job_id}")
            return Result.success(message=DocumentMessage.CREATED.message, code=DocumentMessage.CREATED.code)

        except Exception as e:
            self.logger.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu {job_id}: {str(e)}", exc_info=True)
            await self._update_status_async(
                job_id=job_id,
                status=DocumentJobStatus.FAILED,
                message=f"L·ªói: {str(e)}"
            )
            return Result.failure(message=DocumentMessage.UPLOAD_FAILED.message, code=DocumentMessage.UPLOAD_FAILED.code)
        finally:
            if temp_dir:
                asyncio.create_task(self._cleanup_temp_files_async(temp_dir))

    async def _get_document_job(self, job_id: str) -> DocumentJobModel:
        """L·∫•y th√¥ng tin document job t·ª´ DB"""
        await self._update_status_async(
            job_id=job_id,
            status=DocumentJobStatus.PROCESSING,
            progress=15,
            message="ƒêang x·ª≠ l√Ω t√†i li·ªáu"
        )
        data = await self.collections.document_jobs.find_one({"_id": ObjectId(job_id)})
        return DocumentJobModel.from_dict(data) if data else None

    async def _download_file_async(self, job_id: str, file_path: str) -> Tuple[str, str]:
        """T·∫£i file t·ª´ MinIO v·ªÅ t·∫°m"""
        await self._update_status_async(job_id=job_id, status=DocumentJobStatus.PROCESSING, progress=30, message="ƒêang t·∫£i t·ªáp tin")
        temp_dir = tempfile.mkdtemp()
        temp_path = os.path.join(temp_dir, os.path.basename(file_path))
        bucket, object_path = file_path.split("/", 1)

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, self.minio_manager.get_file, bucket, object_path)

        def write_file():
            with open(temp_path, "wb") as f:
                for chunk in response.stream(32 * 1024):
                    f.write(chunk)

        await loop.run_in_executor(None, write_file)
        return temp_dir, temp_path

    async def _is_duplicate_async(self, job_id: str, temp_path: str) -> bool:
        """Ki·ªÉm tra tr√πng l·∫∑p b·∫±ng hash"""
        await self._update_status_async(job_id=job_id, status=DocumentJobStatus.PROCESSING, progress=40, message="Ki·ªÉm tra tr√πng l·∫∑p")
        loop = asyncio.get_event_loop()
        file_hash = await loop.run_in_executor(None, self.file_hash_utils.calculate_file_hash, temp_path)
        count = await self.collections.documents.count_documents({"file_hash": file_hash})
        if count > 0:
            await self._update_status_async(
                job_id=job_id,
                status=DocumentJobStatus.FAILED,
                message="T√†i li·ªáu ƒë√£ t·ªìn t·∫°i",
                document_status=DocumentStatus.DUPLICATE
            )
            return True
        return False

    async def _parse_content_async(self, job_id: str, temp_path: str):
        """Parse n·ªôi dung file"""
        await self._update_status_async(job_id=job_id, status=DocumentJobStatus.PROCESSING, progress=50, message="Ph√¢n t√≠ch n·ªôi dung")
        return await parse_file(temp_path)

    async def _create_chunks_async(self, job_id: str, content) -> List:
        """Chia nh·ªè n·ªôi dung"""
        await self._update_status_async(job_id=job_id, status=DocumentJobStatus.PROCESSING, progress=65, message="Chia nh·ªè t√†i li·ªáu")
        chunks = await self.chunker.chunk_async(parsed_content=content)
        return chunks

    async def _score_chunks_async(self, job_id: str, chunks: List) -> List[float]:
        """ƒê√°nh ƒëi·ªÉm ti·ªÉu ƒë∆∞·ªùng theo batch"""
        await self._update_status_async(job_id=job_id, status=DocumentJobStatus.PROCESSING, progress=80, message="ƒê√°nh gi√° ƒë·ªô li√™n quan")
        batch_size = 50
        all_scores = []
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            scores = await self.diabetes_classifier.score_chunks(batch)
            all_scores.extend(scores)
            await asyncio.sleep(0)
        return all_scores

    async def _save_document_async(self, job_id: str, document_job: DocumentJobModel, temp_path: str,
                                 chunks: List, scores: List[float]):
        """L∆∞u document v√† chunks (kh√¥ng x√≥a c≈©)"""
        await self._update_status_async(
            job_id=job_id,
            status=DocumentJobStatus.PROCESSING,
            progress=90,
            message="L∆∞u d·ªØ li·ªáu v√†o c∆° s·ªü d·ªØ li·ªáu"
        )

        # T√≠nh hash
        loop = asyncio.get_event_loop()
        file_hash = await loop.run_in_executor(None, self.file_hash_utils.calculate_file_hash, temp_path)

        # C·∫≠p nh·∫≠t ho·∫∑c t·∫°o m·ªõi document (kh√¥ng x√≥a c≈©)
        document = DocumentModel(
            knowledge_id=document_job.knowledge_id,
            title=document_job.title,
            description=document_job.description,
            document_type=DocumentType.UPLOADED,
            file=DocumentFile(
                path=document_job.file.path,
                size_bytes=os.path.getsize(temp_path),
                name=os.path.basename(temp_path),
                type=document_job.file.type,
            ),
            priority_diabetes=sum(scores) / len(scores) if scores else 0.0,
            file_hash=file_hash,
        )
        document.id = document_job.document_id

        # Upsert document
        await self.collections.documents.replace_one(
            {"_id": ObjectId(document.id)},
            document.to_dict(),
            upsert=True
        )

        await self._save_chunks_in_batches_async(document_job, chunks, scores)

        # C·∫≠p nh·∫≠t th·ªëng k√™
        await self._update_knowledge_stats(document_job.knowledge_id, document.file.size_bytes)

    async def _save_chunks_in_batches_async(self, document_job: DocumentJobModel, chunks: List, scores: List[float], batch_size: int = 500):
        """L∆∞u chunks theo batch, kh√¥ng x√≥a c≈©, m·ªói chunk c√≥ _id m·ªõi"""
        total = len(chunks)
        self.logger.info(f"üíæ B·∫Øt ƒë·∫ßu l∆∞u {total} chunks m·ªõi (gi·ªØ chunk c≈©)")

        for i in range(0, total, batch_size):
            batch = chunks[i:i + batch_size]
            batch_dicts = []

            for j, chunk in enumerate(batch):
                index = i + j
                chunk_model = DocumentChunkModel(
                    document_id=document_job.document_id,
                    knowledge_id=document_job.knowledge_id,
                    content=chunk.content,
                    diabetes_score=scores[index],
                    is_active=True
                )
                batch_dicts.append(chunk_model.to_dict())  # dict kh√¥ng c√≥ _id

            try:
                result = await self.collections.document_chunks.insert_many(batch_dicts, ordered=False)
                self.logger.info(f"L∆∞u batch {i//batch_size + 1}: {len(result.inserted_ids)} chunks")
            except Exception as e:
                self.logger.warning(f"L·ªói khi l∆∞u batch: {str(e)}")

    async def _update_knowledge_stats(self, knowledge_id: str, size_bytes: int):
        """C·∫≠p nh·∫≠t th·ªëng k√™ knowledge"""
        await self.collections.knowledges.update_one(
            {"_id": ObjectId(knowledge_id)},
            {"$inc": {"document_count": 1, "total_size_bytes": size_bytes}}
        )

    async def _update_status_async(self, job_id: str, status: DocumentJobStatus, progress: float = None,
                                 message: str = None, priority_diabetes: float = None,
                                 document_status: DocumentStatus = DocumentStatus.NORMAL, file_size: int = None):
        """C·∫≠p nh·∫≠t tr·∫°ng th√°i job"""
        update_fields = {
            "processing_status.status": status,
            "document_status": document_status
        }
        if progress is not None:
            update_fields["processing_status.progress"] = progress
        if message is not None:
            update_fields["processing_status.progress_message"] = message
        if priority_diabetes is not None:
            update_fields["priority_diabetes"] = priority_diabetes
        if file_size is not None:
            update_fields["file.file_size_bytes"] = file_size

        await self.collections.document_jobs.update_one(
            {"_id": ObjectId(job_id)},
            {"$set": update_fields}
        )

    async def _cleanup_temp_files_async(self, temp_dir: str):
        """D·ªçn d·∫πp file t·∫°m"""
        if os.path.exists(temp_dir):
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, shutil.rmtree, temp_dir, True)
            self.logger.info(f"ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {temp_dir}")